---
title: "Tema 11 - Introducción a distribuciones de probabilidad"
author: "Juan Gabriel Gomila & María Santos"
output: 
  ioslides_presentation:
    widescreen: true
    css: JB_style.css
    logo: Imgs/LogoCurso.png
    fig_height: 4
    fig_width: 10
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
```

# Conceptos básicos

## Experimento aleatorio

<l class = "definition">Experimento aleatorio. </l> Experimento que efectuado en las mismas condiciones puede dar lugar a resultados diferentes

<l class = "definition">Suceso elemental. </l> Cada uno de los posibles resultados del experimento aleatorio

<l class = "definition">Espacio muestral. </l> Conjunto $\Omega$ formado por todos los sucesos elementales del experimento aleatorio

<div class = "example">
**Ejemplo**

Lanzar una moneda es un experimento aleatorio

Los sucesos elementales son: sacar cara ($C$) y sacar cruz ($+$)

El espacio muestral de este experimento aleatorio es $\Omega = \{C,+\}$
</div>

## Sucesos

<l class = "definition">Suceso. </l> Subconjunto del espacio muestral

<l class = "definition">Suceso total o seguro. </l> $\Omega$

<l class = "definition">Suceso vacío o imposible. </l> $\emptyset$

<div class = "example">
**Ejemplo**

Lanzar un dado es un experimento aleatorio

Algunos sucesos podrían ser: sacar número par ($\{2,4,6\}$), sacar mayor que 4 ($\{5,6\}$), sacar número múltiplo de 3 ($\{3,6\}$)...

El suceso total de este experimento aleatorio es $\Omega = \{1,2,3,4,5,6\}$

Un ejemplo de suceso imposible de este experimento aleatorio es $\emptyset = \{7\}$ (sacar 7)
</div>


## Sucesos

<l class = "prop">Operaciones con sucesos. </l> Sean $A,B\subseteq \Omega$ sucesos. Entonces,

- $A\cup B$ es el suceso unión (resultados pertenecen a $A$, o a $B$, o a ambos)
- $A\cap B$ es el suceso intersección (resultados pertenecen a $A$ y $B$)
- $A^c$ es el suceso complementario (resultados que no pertenecen a $A$)
- $A-B = A\cap B^c$ es el suceso diferencia (resultados que pertenecen a $A$ pero no a $B$)

<l class = "definition">Sucesos incompatibles. </l> Si $A\cap B = \emptyset$

## Probabilidad

<l class = "definition">Probabilidad de un suceso. </l>Número entre 0 y 1 (ambos incluidos) que mide la expectativa de que se dé este suceso

<div class = "example">
**Ejemplo**

- La probabilidad de sacar un 6 al lanzar un dado estándar no trucado es $\frac{1}{6}$
- La probabilidad de sacar un 6 al lanzar un dado de 4 caras es $0$
- La probabilidad de sacar un 6 al lanzar un dado de 20 caras es $\frac{1}{20}$

</div>

<div class = "aligncenter">
![](Imgs/dado.png)
</div>

## Probabilidad

<l class = "definition">Probabilidad. </l> Sea $\Omega$ el espacio muestral de un experimento aleatorio. Suponiendo que $\Omega$ es **finito**, una probabilidad sobre $\Omega$ es una aplicación $$p: \mathcal{P}(\Omega)\longrightarrow [0,1]$$ que satisface

- $0\le p(A)\le 1 \ \forall A\in\mathcal{P}(\Omega)$
- $p(\Omega) = 1$
- Si $\{A_1,\dots,A_n\}$ son sucesos incompatibles dos a dos ($A_i\cap A_j=\emptyset \ \forall i\ne j$), entonces $$p(A_1\cup\cdots \cup A_n)=p(A_1)+\cdots+p(A_n)$$

<l class = "important">Notación: </l> Si $a\in\Omega$, escribiremos $p(a)$ en vez de $p(\{a\})$

# Variables aleatorias

## Variable aleatoria

<l class = "definition">Variable aleatoria. </l> Una variable aleatoria (v.a.) sobre $\Omega$ es una aplicación $$X: \Omega\longrightarrow \mathbb{R}$$ que asigna a cada suceso elemental $\omega$ un número real $X(\omega)$ 

Puede entenderse como una descripción numérica de los resultados de un experimento aleatorio

<l class = "definition">Dominio de una variable aleatoria. </l> $D_X$, es el conjunto de los valores que puede tomar

## Sucesos de variables aleatorias

Una variable aleatoria puede definir sucesos, de los cuales queremos conocer la probabilidad $p$

- $p(X=a) = p(\{\omega\in\Omega \ |\  X(\omega) = a\})$
- $p(X<b) = p(\{\omega\in\Omega \ |\  X(\omega) < b\})$
- $p(X\le b) = p(\{\omega\in\Omega \ |\  X(\omega) \le b\})$
- $p(a<X) = p(\{\omega\in\Omega \ |\  a<X(\omega)\})$
- $p(a\le X) = p(\{\omega\in\Omega \ |\  a\le X(\omega)\})$
- $p(a\le X\le b) = p(\{\omega\in\Omega \ |\  a\le X(\omega) \le b\})$
- $p(a< X< b) = p(\{\omega\in\Omega \ |\  a< X(\omega) < b\})$
- $p(X\in A) = p(\{\omega\in\Omega \ |\  X(\omega)\in A\})$

## Función de distribución

<l class = "definition">Función de distribución de la v.a. $X$.</l> Es una función  $$F:\mathbb{R}\longrightarrow [0,1]$$ definida por $F(x)=p(X\le x)$


Sea $F$ una función de distribución de una v.a. $X$ y digamos $$F(a^-)=\lim_{x\rightarrow a^-}F(x)$$

- $p(X\le a)=F(a)$
- $p(X<a)=\lim_{b\rightarrow a,\  b<a}p(X\le b) = \lim_{b\rightarrow a,\  b<a} F(b) = F(a^-)$
- $p(X=a) = p(X\le a)-p(X<a)=F(a)-F(a^-)$
- $p(a\le X\le b) = p(X\le b)-p(X< a)=F(b)-F(a^-)$

## Cuantiles

<l class = "definition">Cuantil de orden $p$ de una v.a. $X$.</l> Es el $x_p\in\mathbb{R}$ más pequeño tal que $F(x_p)\ge p$

Nótese que la mediana es el cuantil de orden 0.5

# Variables aleatorias discretas

## Variable aleatoria discreta

<l class = "definition">Variable aleatoria discreta.</l> Una v.a. $X:\Omega\longrightarrow \mathbb{R}$ es discreta cuando $D_X$ es finito o un subconjunto de $\mathbb{N}$ 

<l class = "definition">Función de probabilidad.</l> Es la función $f:\mathbb{R}\longrightarrow[0,1]$ definida por $$f(x) = p(X=x)$$

Nótese que $f(x)=0$ si $x\not\in D_X$. Por tanto, interpretaremos la función de probabilidad como la función $$f:D_X\longrightarrow [0,1]$$

## Esperanza

<l class = "definition">Esperanza de una v.a. discreta.</l> Sea $f:D_X\longrightarrow[0,1]$ la función de probabilidad de $X$, entonces la esperanza respecto de la función de probabilidad es la suma ponderada de los elementos de $D_X$, multiplicando cada elemento $x$ de $D_X$ por su probabilidad, $$E(X) = \sum_{x\in D_X}x\cdot f(x)$$

Si $g:D_X\longrightarrow \mathbb{R}$ es una aplicación $$E(g(X))=\sum_{x\in D_X}g(x)\cdot f(x)$$


## Varianza

<l class = "definition">Varianza de una v.a. discreta.</l> Sea $f:D_X\longrightarrow[0,1]$ la función de probabilidad de $X$, entonces la varianza respecto de la función de probabilidad es el valor esperado de la diferencia al cuadrado entre $X$ y su valor medio $E(X)$, $$Var(X)= E((X-E(X))^2) $$

La varianza mide como de variados son los resultados de $X$ respecto de la media

<div class = "exercise"> **Ejercicio.** Demostrar la siguiente igualdad. $$Var(X)= E(X^2)-(E(X))^2$$</div>

## Varianza

Si $X$ es una v.a. discreta y $g:D_X\longrightarrow \mathbb{R}$ una función, $$Var(g(X))=E((g(X)-E(g(X)))^2)=E(g(X)^2)-(E(g(X)))^2$$

## Desviación típica

<l class = "definition">Desviación típica de una v.a. discreta.</l> Sea $f:D_X\longrightarrow[0,1]$ la función de probabilidad de $X$, entonces la desviación típica respecto de la función de probabilidad es $$\sigma(X)=\sqrt{Var(X)}$$

Las unidades de la varianza son las de $X$ al cuadrado. En cambio, las de la desviación típica son las mismas unidades que las de $X$

Si $X$ es una v.a. discreta y $g:D_X\longrightarrow \mathbb{R}$ una función, $$\sigma(g(X))=\sqrt{Var(g(X))}$$

# Distribuciones de probabilidad

## Distribución de probabilidad

<l class = "definition">[Distribución de probabilidad](https://es.wikipedia.org/wiki/Distribución_de_probabilidad).</l> En teoría de la probabilidad y estadística, la distribución de probabilidad de una variable aleatoria es una función que asigna a cada suceso definido sobre la variable la probabilidad de que dicho suceso ocurra.

## Distribuciones en `R`

Dada cualquier variable aleatoria, `va`, `R` nos da cuatro funciones para poder trabajar con ellas:

- `dva(x,...)`: Función de densidad o de probabilidad $f(x)$ de la variable aleatoria para el valor  $x$ del dominio de definición.
- `pva(x,...)`: Función de distribución $F(x)$ de la variable aleatoria para el valor $x$ del dominio de definición.
- `qva(p,...)`: Cuantil $p$-ésimo de la variable aleatoria (el valor de $x$ más pequeño tal que $F(x)\geq p$).
- `rva(n,...)`: Generador de $n$ observaciones siguiendo la distribución de la variable aleatoria.

## Distribuciones en `Python`

Dada cualquier variable aleatoria, en `Python` tenemos las mismas cuatro funciones, sin que su nombre dependa de la misma:

- `pmf(k,...)` o `pdf(x,...)`: Función de probabilidad $f(k)$ o de densidad $f(x)$ de la variable aleatoria para los valores $k$ o $x$ del dominio.
- `cdf(x,...)`: Función de distribución $F(x)$ de la variable aleatoria para el valor $k$ del dominio.
- `ppf(p,...)`: Cuantil $p$-ésimo de la variable aleatoria (el valor de $x$ más pequeño tal que $F(x)\geq p$).
- `rvs(size,...)`: Generador de $size$ observaciones siguiendo la distribución de la variable aleatoria.

También vale la pena conocer la función `stats(moments='mvsk')` que nos devuelve cuatro valores con los estadísticos de la media `m`, la varianza `v`, el sesgo `s` y la curtosis `k` de la distribución.

# Distribuciones discretas más conocidas

## Distribuciones discretas 

- [Bernoulli](https://es.wikipedia.org/wiki/Distribución_de_Bernoulli)
- [Binomial](https://es.wikipedia.org/wiki/Distribución_binomial)
- [Geométrica](https://es.wikipedia.org/wiki/Distribución_geométrica)
- [Hipergeométrica](https://es.wikipedia.org/wiki/Distribución_hipergeométrica)
- [Poisson](https://es.wikipedia.org/wiki/Distribución_de_Poisson)
- [Binomial Negativa](https://es.wikipedia.org/wiki/Distribución_binomial_negativa)

## Distribución de Bernoulli

Si $X$ es variable aleatoria que mide el "número de éxitos" y se realiza un único experimento con dos posibles resultados (éxito, que toma valor 1, o fracaso, que toma valor 0), diremos que $X$ se distribuye como una Bernoulli con parámetro $p$

$$X\sim \text{Be}(p)$$

donde $p$ es la probabilidad de éxito y $q = 1-p$ es la probabilidad de fracaso.

- El **dominio** de $X$ será $D_X = \{0,1\}$
- La **función de probabilidad** vendrá dada por $$f(k) = p^k(1-p)^{1-k} =  \left\{
\begin{array}{rl}
     p & \text{si } k=1 
  \\ 1-p & \text{si } k=0
  \\ 0 & \text{en cualquier otro caso}
\end{array}
\right.$$

## Distribución de Bernoulli

- La **función de distribución** vendrá dada por $$F(x) = \left\{
\begin{array}{rl}
     0 & \text{si } x<0 
  \\ 1-p & \text{si } 0\le x<1
  \\ 1 & \text{si } x\ge 1
\end{array}
\right.$$
- **Esperanza** $E(X) = p$
- **Varianza** $Var(X) = pq$

## Distribución de Bernoulli

El código de la distribución de Beroulli:

- En `R` tenemos las funciones del paquete `Rlab`: `dbenr(x,prob), pbenr(q,prob), qbenr(p,prob), rbenr(n, prob)` donde `prob` es la probabilidad de éxito.
- En `Python` tenemos las funciones del paquete `scipy.stats.bernoulli`: `pmf(k,p), cdf(k,p), ppf(q,p), rvs(p, size)` donde `p` es la probabilidad de éxito.

## Distribución Binomial

Si $X$ es variable aleatoria que mide el "número de éxitos" y se realizan $n$ ensayos de Bernoulli independientes entre sí, diremos que $X$ se distribuye como una Binomial con parámetros $n$ y $p$

$$X\sim \text{B}(n,p)$$

donde $p$ es la probabilidad de éxito y $q = 1-p$ es la probabilidad de fracaso

- El **dominio** de $X$ será $D_X = \{0,1,2,\dots,n\}$
- La **función de probabilidad** vendrá dada por $$f(k) = {n\choose k}p^k(1-p)^{n-k} $$

## Distribución Binomial

- La **función de distribución** vendrá dada por $$F(x) = \left\{
\begin{array}{cl}
     0 & \text{si } x<0 
  \\ \sum_{k=0}^xf(k) & \text{si } 0\le x<n
  \\ 1 & \text{si } x\ge n
\end{array}
\right.$$
- **Esperanza** $E(X) = np$
- **Varianza** $Var(X) = npq$

<l class = "important">Atención.</l> Fijaos que la distribución de Bernoulli es un caso particular de la Binomial. Basta tomar $n=1$ y tendremos que $X\sim \text{Be}(p)$ y $X\sim\text{B}(1,p)$ son equivalentes.

## Distribución Binomial

```{r, echo = FALSE}
par(mfrow = c(1,2))
plot(0:50,dbinom(0:50,50,0.5),col = "purple", xlab = "", ylab = "", main = "Función de probabilidad de una B(50,0.5)")
plot(0:50, pbinom(0:50,50,0.5),col = "purple", xlab = "", ylab = "", main = "Función de distribución de una B(50,0.5)", ylim = c(0,1))
par(mfrow= c(1,1))
```

## Distribución Binomial

El código de la distribución Binomial:

- En `R` tenemos las funciones del paquete `Rlab`: `dbinom(x, size, prob), pbinom(q,size, prob), qbinom(p, size, prob), rbinom(n, size, prob)` donde `prob` es la probabilidad de éxito y `size` el número de ensayos del experimento.
- En `Python` tenemos las funciones del paquete `scipy.stats.binom`: `pmf(k,n,p), cdf(k,n,p), ppf(q,n,p), rvs(n, p, size)` donde `p` es la probabilidad de éxito y `n` el número de ensayos del experimento.


## Distribución Geométrica

Si $X$ es variable aleatoria que mide el "número de repeticiones independientes del experimento hasta haber conseguido éxito", diremos que $X$ se distribuye como una Geométrica con parámetro $p$

$$X\sim \text{Ge}(p)$$
donde $p$ es la probabilidad de éxito y $q = 1-p$ es la probabilidad de fracaso

- El **dominio** de $X$ será $D_X= \{0,1,2,\dots\}$ o bien $D_X = \{1,2,\dots\}$ en función de si empieza en 0 o en 1, respectivamente

- La **función de probabilidad** vendrá dada por $$f(k) = (1-p)^{k}p \qquad\text{ si empieza en 0}$$
$$f(k) = (1-p)^{k-1}p \qquad\text{ si empieza en 1}$$

## Distribución Geométrica

- La **función de distribución** vendrá dada por $$F(x) = \left\{
\begin{array}{cl}
     0 & \text{si } x<0 
  \\ 1-(1-p)^{k+1} & \text{si } k\le x<k+1,\ k\in\mathbb{N}
\end{array}
\right.$$ 
- **Esperanza** $E(X) = \frac{1-p}{p}$ si empieza en 0 y E$(X) = \frac{1}{p}$ si empieza en 1
- **Varianza** $Var(X) = \frac{1-p}{p^2}$
- <l class = "prop">Propiedad de la falta de memoria.</l> Si $X$ es una v.a. $\text{Ge}(p)$, entonces, $$p\{X\ge m+n:\ X\ge n\} = p\{X\ge m\}\ \forall m,n=0,1,\dots$$

## Distribución Geométrica

```{r, echo = FALSE}
par(mfrow = c(1,2))
plot(0:20, dgeom(0:20,0.5),col = "purple", xlab = "", ylab = "", main = "Función de probabilidad de una Ge(0.5)")
plot(0:20, pgeom(0:20,0.5),col = "purple", xlab = "", ylab = "", main = "Función de distribución de una Ge(0.5)", ylim = c(0,1))
par(mfrow= c(1,1))
```

## Distribución Geométrica

El código de la distribución Geométrica:

- En `R` tenemos las funciones del paquete `Rlab`: `dgeom(x, prob), pgeom(q, prob), qgeom(p, prob), rgeom(n, prob)` donde `prob` es la probabilidad de éxito  del experimento.
- En `Python` tenemos las funciones del paquete `scipy.stats.geom`: `pmf(k,p), cdf(k,p), ppf(q,p), rvs(p, size)` donde `p` es la probabilidad de éxito del experimento.

## Distribución Hipergeométrica

Consideremos el experimento "extraer a la vez (o una detrás de otra, sin retornarlos) $n$ objetos donde hay $N$ de tipo A y $M$ de tipo B". Si $X$ es variable aleatoria que mide el "número de objetos del tipo A", diremos que $X$ se distribuye como una Hipergeométrica con parámetros $N,M,n$
$$X\sim \text{H}(N,M,n)$$

- El **dominio** de $X$ será $D_X = \{0,1,2,\dots,N\}$ (en general)
- La **función de probabilidad** vendrá dada por $$f(k) = \frac{{N\choose k}{M\choose n-k}}{N+M\choose n}$$

## Distribución Hipergeométrica

- La **función de distribución** vendrá dada por $$F(x) = \left\{
\begin{array}{cl}
     0 & \text{si } x<0 
  \\ \sum_{k=0}^xf(k) & \text{si } 0\le x<n
  \\ 1 & \text{si } x\ge n
\end{array}
\right.$$
- **Esperanza** $E(X) = \frac{nN}{N+M}$ 
- **Varianza** $Var(X) = \frac{nNM}{(N+M)^2}\cdot\frac{N+M-n}{N+M-1}$

## Distribución Hipergeométrica

```{r, echo = FALSE}
par(mfrow = c(1,2))
plot(0:30, dhyper(0:30,10,20,10),col = "purple", xlab = "", ylab = "", main = "Función de probabilidad de una H(20,10,30)")
plot(0:30, phyper(0:30,10,20,10),col = "purple", xlab = "", ylab = "", main = "Función de distribución de una H(20,10,30)", ylim = c(0,1))
par(mfrow= c(1,1))
```

## Distribución Hipergeométrica

El código de la distribución Hipergeométrica:

- En `R` tenemos las funciones del paquete `Rlab`: `dhyper(x, m, n, k), phyper(q,  m, n, k), qhyper(p,  m, n, k), rhyper(nn,  m, n, k)` donde `m` es el número de objetos del primer tipo, `n` el número de objetos del segundo tipo y `k` el número de extracciones realizadas.
- En `Python` tenemos las funciones del paquete `scipy.stats.hypergeom`: `pmf(k,M, n, N), cdf(k,M, n, N), ppf(q,M, n, N), rvs(M, n, N, size)` donde `M` es el número de objetos del primer tipo, `N` el número de objetos del segundo tipo y `n` el número de extracciones realizadas.

## Distribución de Poisson

Si $X$ es variable aleatoria que mide el "número de eventos en un cierto intervalo de tiempo", diremos que $X$ se distribuye como una Poisson con parámetro $\lambda$

$$X\sim \text{Po}(\lambda)$$
donde $\lambda$ representa el número de veces que se espera que ocurra el evento durante un intervalo dado

- El **dominio** de $X$ será $D_X = \{0,1,2,\dots\}$

- La **función de probabilidad** vendrá dada por $$f(k) = \frac{e^{-\lambda}\lambda^k}{k!}$$

## Distribución de Poisson
 
- La **función de distribución** vendrá dada por $$F(x) = \left\{
\begin{array}{cl}
     0 & \text{si } x<0 
  \\ \sum_{k=0}^xf(k) & \text{si } 0\le x<n
  \\ 1 & \text{si } x\ge n
\end{array}
\right.$$ 
- **Esperanza** $E(X) = \lambda$
- **Varianza** $Var(X) = \lambda$

## Distribución de Poisson

```{r, echo = FALSE}
par(mfrow = c(1,2))
plot(0:20, dpois(0:20,2),col = "purple", xlab = "", ylab = "", main = "Función de probabilidad de una Po(2)")
plot(0:20, ppois(0:20,2),col = "purple", xlab = "", ylab = "", main = "Función de distribución de una Po(2)", ylim = c(0,1))
par(mfrow= c(1,1))
```

## Distribución de Poisson

El código de la distribución de Poisson:

- En `R` tenemos las funciones del paquete `Rlab`: `dpois(x, lambda), ppois(q,lambda), qpois(p,lambda), rpois(n, lambda)` donde `lambda` es el número esperado de eventos por unidad de tiempo de la distribución.
- En `Python` tenemos las funciones del paquete `scipy.stats.poisson`: `pmf(k,mu), cdf(k,mu), ppf(q,mu), rvs(M,mu)` donde `mu` es el número esperado de eventos por unidad de tiempo de la distribución.

## Distribución Binomial Negativa

Si $X$ es variable aleatoria que mide el "número de repeticiones hasta observar los $r$ éxitos en ensayos de Bernoulli", diremos que $X$ se distribuye como una Binomial Negativa con parámetros $r$ y $p$, $$X\sim\text{BN}(r,p)$$ donde $p$ es la probabilidad de éxito

- El **dominio** de $X$ será $D_X = \{r, r+1, r+2,\dots\}$
- La **función de probabilidad** vendrá dada por $$f(k) = {k-1\choose r-1}p^r(1-p)^{k-r}, k\geq r$$


## Distribución Binomial Negativa
 
- La **función de distribución** no tiene una expresión analítica. 
- **Esperanza** $E(X) = \frac{r}{p}$
- **Varianza** $Var(X) = r\frac{1-p}{p^2}$

## Distribución Binomial Negativa

```{r, echo = FALSE}
par(mfrow = c(1,2))
exitos = 5
size = 20
plot(c(rep(0,exitos),exitos:(size+exitos)), c(rep(0,exitos),dnbinom(0:size,exitos,0.5)),col = "purple", xlab = "", ylab = "", main = "Función de probabilidad de una BN(5, 0.5)")
plot(c(rep(0,exitos),exitos:(size+exitos)), c(rep(0,exitos),pnbinom(0:size,exitos,0.5)),col = "purple", xlab = "", ylab = "", main = "Función de distribución de una BN(5, 0.5)")
par(mfrow= c(1,1))
```

## Distribución  Binomial Negativa

El código de la distribución Binomial Negativa:

- En `R` tenemos las funciones del paquete `Rlab`: `dnbinom(x, size, prop), pnbinom(q, size, prop), qnbinom(p, size, prop), rnbinom(n, size, prop)` donde `size` es el número de casos exitosos y `prob` la probabilidad del éxito.
- En `Python` tenemos las funciones del paquete `scipy.stats.nbinom`: `pmf(k,n,p), cdf(k,n,p), ppf(q,n,p), rvs(n,p)` donde `n`es el número de casos exitosos y `p` la probabilidad del éxito.

## Distribuciones discretas en R

R conoce las distribuciones de probabilidad más importantes.

Distribución |  Instrucción en R  |  Instrucción en Python  |  Parámetros                                
--------------------|--------------------|--------------------|--------------------
Bernoulli | `bern` | `scipy.stats.bernoulli` | probabilidad de éxito $p$
Binomial | `binom` | `scipy.stats.binom` | tamaño de la muestra $n$ y probabilidad de éxito $p$
Geométrica | `geom` | `scipy.stats.geom` | probabilidad de éxito $p$
Hipergeométrica | `hyper` | `scipy.stats.hypergeom` | $N,M,n$
Poisson | `pois` | `scipy.stats.poisson` | esperanza $\lambda$
Binomial Negativa | `nbinom` | `scipy.stats.nbinom` | número de éxitos $r$ y probabilidad de éxito $p$


# Variables aleatorias continuas

## Variable aleatoria continua

<l class = "definition">Variable aleatoria continua.</l> Una v.a. $X:\Omega\longrightarrow\mathbb{R}$ es continua cuando su función de distribución $F_X:\mathbb{R}\longrightarrow[0,1]$ es continua

En este caso, $F_X(x)=F_X(x^-)$ y, por este motivo, $$p(X=x)=0\ \forall x\in\mathbb{R}$$
pero esto no significa que sean sucesos imposibles

## Función de densidad

<l class = "definition">Función de densidad.</l> Función $f:\mathbb{R}\longrightarrow\mathbb{R}$ que satisface 

- $f(x)\ge 0\ \forall x\in\mathbb{R}$
- $\int_{-\infty}^{+\infty}f(t)dt=1$

Una función de densidad puede tener puntos de discontinuidad

## Variable aleatoria continua

Toda variable aleatoria $X$ con función de distribución 

$$F(x)=\int_{-\infty}^{x}f(t)dt\ \forall x\in\mathbb{R}$$ para cualquier densidad $f$ es una v.a. continua

Diremos entonces que $f$ es la función de densidad de $X$

A partir de ahora, considerareos solamente las v.a. $X$ continuas que tienen función de densidad


## Esperanza

<l class = "definition">Esperanza de una v.a. continua.</l> Sea $X$ v.a. continua con densidad $f_X$. La esperanza de $X$ es $$E(X)=\int_{-\infty}^{+\infty}x\cdot f_X(x)dx$$

Si el dominio $D_X$ de $X$ es un intervalo de extremos $a<b$, entonces $$E(X)=\int_a^b x\cdot f_X(x)dx$$

## Esperanza

Sea $g:D_X\longrightarrow \mathbb{R}$ una función continua. Entonces, 

$$E(g(X)) = \int_{-\infty}^{+\infty}g(x)\cdot f_X(x)dx$$

Si el dominio $D_X$ de $X$ es un intervalo de extremos $a<b$, entonces $$E(g(X))=\int_a^b g(x)\cdot f_X(x)dx$$

## Varianza

<l class = "definition">Varianza de una v.a. continua.</l> Como en el caso discreto, $$Var(X)=E((X-E(X))^2)$$

y se puede demostrar que

$$Var(X)=E(X^2)-(E(X))^2$$

## Desviación típica

<l class = "definition">Desviación típica de una v.a. continua.</l> Como en el caso discreto, $$\sigma = \sqrt{Var(X)}$$

# Distribuciones continuas más conocidas

## Distribuciones continuas

- [Uniforme](https://es.wikipedia.org/wiki/Distribución_uniforme_continua)
- [Exponencial](https://es.wikipedia.org/wiki/Distribución_exponencial)
- [Normal](https://es.wikipedia.org/wiki/Distribución_normal)
- [Khi cuadrado](https://es.wikipedia.org/wiki/Distribución_χ²)
- [t de Student](https://es.wikipedia.org/wiki/Distribución_t_de_Student)
- [F de Fisher](https://es.wikipedia.org/wiki/Distribución_F)


## Distribución Uniforme

Una v.a. continua $X$ tiene distribución uniforme sobre el intervalo real $[a,b]$ con $a<b$, $X\sim\text{U}(a,b)$ si su función de densidad es $$f_X(x)=\left\{
\begin{array}{rl}
     \frac{1}{b-a} & \text{si } a\le x\le b
  \\ 0 & \text{en cualquier otro caso}
\end{array}
\right.$$

Modela el elegir un elemento del intervalo $[a,b]$ de manera equiprobable

## Distribución Uniforme

- El **dominio** de $X$ será $D_X = [a,b]$

- La **función de distribución** vendrá dada por $$F_X(x)=\left\{
\begin{array}{rl}
    0 & \text{si } x<a
  \\ \frac{x-a}{b-a} & \text{si } a\le x< b
  \\ 1 & \text{si } x\ge b
\end{array}
\right.$$

- **Esperanza** $E(X) = \frac{a+b}{2}$
- **Varianza** $Var(X) = \frac{(b-a)^2}{12}$

## Distribución Uniforme

```{r, echo = FALSE}
par(mfrow=c(1,2))
plot(c(0,1,1:4,4,5), c(0,0,dunif(1:4,min = 1, max = 4),0,0),col = "purple", xlab = "", ylab = "", main = "Función de densidad de una U(1,4)", type = "o", ylim = c(0,1))
plot(0:5, punif(0:5,min = 1, max = 4),col = "purple", xlab = "", ylab = "", main = "Función de distribución de una U(1,4)", type = "o")
par(mfrow=c(1,1))
```


## Distribución Uniforme

El código de la distribución Uniforme:

- En `R` tenemos las funciones del paquete `stats`: `dunif(x, min, max), punif(q, min, max), qunif(p, min, max), runif(n,  min, max)` donde `min` y `max` són los extremos de los intervalos de la distribución uniforme.
- En `Python` tenemos las funciones del paquete `scipy.stats.uniform`: `pdf(k,loc, scale), cdf(k,loc, scale), ppf(q,loc, scale), rvs(n,loc, scaler)` donde la distribución uniforme está definida en el intervalo `[loc, loc+scale]`.

## Distribución Exponencial

Una v.a. $X$ tiene distribución exponencial de parámetro $\lambda$, $X\sim\text{Exp}(\lambda)$, si su función de densidad es $$f_X(x)=\left\{
\begin{array}{rl}
     0 & \text{si }  x\le 0
  \\ \lambda\cdot e^{-\lambda x} & \text{si }x>0
\end{array}
\right.$$

<l class = "prop">Teorema. </l> Si tenemos un proceso de Poisson de parámetro $\lambda$ por unidad de tiempo, el tiempo que pasa entre dos sucesos consecutivos es una v.a. $\text{Exp}(\lambda)$ 

<l class = "prop">Propiedad de la pérdida de memoria. </l> Si $X$ es v.a. $\text{Exp}(\lambda)$, entonces $$p(X>s+t\ :\ X>s)=p(X>t)\ \forall s,t>0$$

## Distribución Exponencial

- El **dominio** de $X$ será $D_X = [0,\infty)$

- La **función de distribución** vendrá dada por $$F_X(x)=\left\{
\begin{array}{rl}
    0 & \text{si } x\le 0
  \\ 1-e^{-\lambda x} & \text{si } x>0
\end{array}
\right.$$

- **Esperanza** $E(X) = \frac{1}{\lambda}$
- **Varianza** $Var(X) = \frac{1}{\lambda^2}$

## Distribución Exponencial

```{r, echo = FALSE}
par(mfrow = c(1,2))
plot(0:20, dexp(0:20,0.2),col = "purple", xlab = "", ylab = "", main = "Función de densidad de una Exp(0.2)", type = "o")
plot(0:20, pexp(0:20,0.2),col = "purple", xlab = "", ylab = "", main = "Función de distribución de una Exp(0.2)", type = "o", ylim = c(0,1))
par(mfrow = c(1,1))
```


## Distribución Exponencial

El código de la distribución Exponencial:

- En `R` tenemos las funciones del paquete `stats`: `dexp(x, rate), pexp(q, rate), qexp(p, rate), rexp(n,  rate)` donde `rate`$=\lambda$ es el tiempo entre dos sucesos consecutivos de la distribución.
- En `Python` tenemos las funciones del paquete `scipy.stats.expon`: `pdf(k, scale), cdf(k, scale), ppf(q, scale), rvs(n, scaler)` donde `scale`$=1/\lambda$ es la inversa del tiempo entre dos sucesos consecutivos de la distribución.


## Distribución Normal

Una v.a. $X$ tiene distribución normal o gaussiana de parámetros $\mu$ y $\sigma$, $X\sim\mathcal{N}(\mu,\sigma)$ si su función de densidad es $$f_X(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\quad \forall x\in\mathbb{R}$$

La gráfica de $f_X$ es conocida como la <l class = "definition">Campana de Gauss</l>

Cuando $\mu = 0$ y $\sigma = 1$, diremos que la v.a. $X$ es <l class = "definition">estándar</l> y la indicaremos usualmente como $Z$, la cual tendrá función de densidad
$$f_Z(z)=\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}\quad \forall z\in\mathbb{R}$$

## Distribución Normal

- **Esperanza** $E(X) = \mu$
- **Varianza** $Var(X) = \sigma^2$

En particualr, si $Z$ sigue una distribución estándar,

- **Esperanza** $E(X) = 0$
- **Varianza** $Var(X) = 1$

## Distribución Normal

```{r, echo = FALSE}
par(mfrow = c(1,2))
z_scores <- seq(-10, 10, by = .1)
dvalues <- dnorm(z_scores)
plot(z_scores, dvalues, ylab = "", xlab= "",
     type = "l", 
     col = "purple",
     main = "Función de densidad de una N(0,1)")
dvalues <- pnorm(z_scores)
plot(z_scores, dvalues, ylab = "", xlab= "",
     type = "l", 
     col = "purple",
     main = "Función de distribución de una N(0,1)", ylim = c(0,1))
par(mfrow = c(1,1))
```

## Distribución Normal

El código de la distribución Normal:

- En `R` tenemos las funciones del paquete `stats`: `dnorm(x, mean, sd), pnorm(q,  mean, sd), qnorm(p,  mean, sd), rnorm(n,   mean, sd)` donde `mean` es la media y `sd` es la desviación estándar de la normal $N(\mu, \sigma)$.
- En `Python` tenemos las funciones del paquete `scipy.stats.normal`: `pdf(k, mu, scale), cdf(k,  mu, scale), ppf(q,  mu, scale), rvs(n,  mu, scale)`  donde `mu` es la media y `scale` es la desviación estándar de la normal $N(\mu, \sigma)$.


## Distribución Normal

<l class = "prop">Estandarización de una v.a. normal.</l> Si $X$ es una v.a. $\mathcal{N}(\mu,\sigma)$, entonces $$Z=\frac{X-\mu}{\sigma}\sim\mathcal{N}(0,1)$$

Las probabilidades de una normal estándar $Z$ determinan las de cualquier $X$ de tipo $\mathcal{N}(\mu,\sigma)$:

$$p(X\le x)=p\left(\frac{X-\mu}{\sigma}\le\frac{x-\mu}{\sigma}\right)=p\left(Z\le\frac{x-\mu}{\sigma}\right)$$

## Distribución Normal

$F_Z$ no tiene expresión conocida.

Se puede calcular con cualquier programa, como por ejemplo R, o bien a mano utilizando las [tablas de la $\mathcal{N}(0,1)$](https://github.com/joanby/r-basic/blob/master/teoria/TablaNormal.pdf)

Con las tablas se pueden calcular tanto probabilidades como cuantiles

## Distribución Normal en R y Python

Si a la hora de llamar a alguna de las 4 funciones siguientes: `dnorm`, `pnorm`, `qnorm` o `rnorm` no especificásemos los parámetros de  la media ni la desviación típica, R entiende que se trata de la normal estándar: la $\mathcal{N}(0,1)$.

Es decir, R interpreta $\mu = 0$ y $\sigma = 1$

En Python ocurre exactamente lo mismo.

## Otras distribuciones importantes

- La distribución $\chi^2_k$, donde $k$ representa los grados de libertad de la misma y que procede de la suma de los cuadrados de $k$ distribuciones normales estándar independientes:

$$X = Z_1^2 + Z_2^2+\cdots + Z_k^2\sim \chi_k^2$$

## Otras distribuciones importantes

- La distribución $t_k$ surge del problema de estimar la media de una población normalmente distribuida cuando el tamaño de la muestra es pequeña y procede del cociente

$$T = \frac{Z}{\sqrt{\chi^2_k/k}}\sim T_k$$

## Otras distribuciones importantes

- La distribución $F_{n_1,n_2}$ aparece frecuentemente como la distribución nula de una prueba estadística, especialmente en el análisis de varianza. Viene definida como el cociente

$$F = \frac{\chi^2_{n_1}/n_1}{\chi^2_{n_2}/n_2}\sim F_{n_1,n_2}$$

## Distribuciones continuas en R

Distribución |  Instrucción en R |  Instrucción en Python |  Parámetros                                
--------------------|--------------------|--------------------|--------------------
Uniforme | `unif` | `scipy.stats.uniform` | mínimo y máximo
Exponencial | `exp` | `scipy.stats.expon` | $\lambda$
Normal | `norm` | `scipy.stats.normal` | media $\mu$, desviación típica $\sigma$
Khi cuadrado | `chisq` | `scipy.stats.chi2` | grados de libertad
t de Student | `t` | `scipy.stats.t` | grados de libertad
F de Fisher | `f` | `scipy.stats.f` | los dos grados de libertad

## Otras distribuciones conocidas

- Distribución de Pareto (Power Law)
- Distribución Gamma y Beta
- Distribución Log Normal
- Distribución de Weibull
- Distribución de Cauchy
- Distribución Exponencial Normal
- Distribución Von Mises
- Distribución Rayleigh
- ...
